{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2JfPNSCJHwYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Dummy data\n",
        "dummy_data = [\n",
        "    {\n",
        "        \"date\": \"2023-12-25\",\n",
        "        \"outfit\": {\n",
        "            \"color\": \"Grey\",\n",
        "            \"pattern\": \"Solid\",\n",
        "            \"material\": \"Fleece\",\n",
        "            \"occasion\": \"Casual\",\n",
        "            \"accessories\": [\"Sneakers\", \"Backpack\"],\n",
        "            \"season\": \"Winter\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"date\": \"2023-07-04\",\n",
        "        \"outfit\": {\n",
        "            \"color\": \"Red\",\n",
        "            \"pattern\": \"Striped\",\n",
        "            \"material\": \"Cotton\",\n",
        "            \"occasion\": \"Formal\",\n",
        "            \"accessories\": [\"Hat\", \"Watch\"],\n",
        "            \"season\": \"Summer\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"date\": \"2023-11-23\",\n",
        "        \"outfit\": {\n",
        "            \"color\": \"Brown\",\n",
        "            \"pattern\": \"Plaid\",\n",
        "            \"material\": \"Wool\",\n",
        "            \"occasion\": \"Casual\",\n",
        "            \"accessories\": [\"Scarf\", \"Boots\"],\n",
        "            \"season\": \"Fall\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"date\": \"2023-05-01\",\n",
        "        \"outfit\": {\n",
        "            \"color\": \"Blue\",\n",
        "            \"pattern\": \"Floral\",\n",
        "            \"material\": \"Linen\",\n",
        "            \"occasion\": \"Casual\",\n",
        "            \"accessories\": [\"Sunglasses\", \"Sandals\"],\n",
        "            \"season\": \"Spring\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"date\": \"2023-09-10\",\n",
        "        \"outfit\": {\n",
        "            \"color\": \"Green\",\n",
        "            \"pattern\": \"Camouflage\",\n",
        "            \"material\": \"Polyester\",\n",
        "            \"occasion\": \"Outdoor\",\n",
        "            \"accessories\": [\"Cap\", \"Boots\"],\n",
        "            \"season\": \"Fall\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Save dummy data to a file\n",
        "with open('fashion_trends_dummy.json', 'w') as f:\n",
        "    json.dump(dummy_data, f)\n",
        "\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, max_length=512):\n",
        "        self.data = json.load(open(data_path, 'r'))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        date = item['date']\n",
        "        outfit = json.dumps(item['outfit'])\n",
        "        inputs = self.tokenizer(date, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        outputs = self.tokenizer(outfit, max_length=self.max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        input_ids = inputs.input_ids.squeeze()\n",
        "        attention_mask = inputs.attention_mask.squeeze()\n",
        "        labels = outputs.input_ids.squeeze()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100  # Replace padding token id's with -100\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "    labels = torch.stack([item['labels'] for item in batch])\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "def fine_tune_model(data_path, model_save_path, epochs=5, batch_size=4, lr=5e-5):\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "    dataset = FashionDataset(data_path, tokenizer)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "    model.save_pretrained(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "def generate_outfit(model_path, date):\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "    inputs = tokenizer(date, return_tensors=\"pt\")\n",
        "    output_sequences = model.generate(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask, max_new_tokens=100)\n",
        "    predicted_outfit = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    # Debugging: Print the raw output\n",
        "    print(f\"Raw model output: {predicted_outfit}\")\n",
        "\n",
        "    try:\n",
        "        outfit_dict = json.loads(predicted_outfit)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON decode error: {e}\")\n",
        "        return None\n",
        "    return outfit_dict\n",
        "\n",
        "# Fine-tuning the model\n",
        "data_path = 'fashion_trends_dummy.json'  # Path to the dummy dataset\n",
        "model_save_path = 'fashion_trend_model'  # Path to save the fine-tuned model\n",
        "fine_tune_model(data_path, model_save_path)\n",
        "\n",
        "# Generating an outfit\n",
        "future_date = \"2024-12-25\"\n",
        "predicted_outfit = generate_outfit(model_save_path, future_date)\n",
        "print(predicted_outfit)\n",
        "\n"
      ],
      "metadata": {
        "id": "LQcfXV3tXIL0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10632a81-87d3-414c-d886-ac55ef787967"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 6.041658401489258\n",
            "Epoch: 0, Loss: 11.242891311645508\n",
            "Epoch: 1, Loss: 7.000406742095947\n",
            "Epoch: 1, Loss: 5.712445259094238\n",
            "Epoch: 2, Loss: 5.989035606384277\n",
            "Epoch: 2, Loss: 4.337033748626709\n",
            "Epoch: 3, Loss: 5.586666107177734\n",
            "Epoch: 3, Loss: 4.494311809539795\n",
            "Epoch: 4, Loss: 4.783902645111084\n",
            "Epoch: 4, Loss: 3.7604734897613525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw model output: 2024-12-25\n",
            "JSON decode error: Extra data: line 1 column 5 (char 4)\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v7bOXBXrnZUP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}